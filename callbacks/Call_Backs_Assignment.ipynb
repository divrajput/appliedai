{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2ZRzYGsCSnt"
   },
   "source": [
    "\n",
    "### 1. Download the data from <a href='https://drive.google.com/file/d/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM/view?usp=sharing'>here</a>. You have to use data.csv file for this assignment\n",
    "### 2. Code the model to classify data like below image. You can use any number of units in your Dense layers.\n",
    "\n",
    "<img src='https://i.imgur.com/33ptOFy.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg22rD7sDPDu"
   },
   "source": [
    "# <font color='red'> <b>3. Writing Callbacks </b> </font>\n",
    "## You have to implement the following callbacks\n",
    "-  Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.Do not use tf.keras.metrics for calculating AUC and F1 score.\n",
    "\n",
    "- Save your model at every epoch if your validation accuracy is improved from previous epoch. \n",
    "\n",
    "- You have to decay learning based on below conditions \n",
    "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
    "               learning rate by 10%. \n",
    "        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n",
    "        \n",
    "- If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n",
    "\n",
    "- You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n",
    "\n",
    "- Use tensorboard for every model and analyse your scalar plots and histograms. (you need to upload the screenshots and write the observations for each model for evaluation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCL3OGS0DsUa"
   },
   "source": [
    "<pre>\n",
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZaRHRdEHDzOM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tensorflow.keras.layers import Dense,Input,Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import random as rn\n",
    "from tensorflow import keras\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CSw5K9mrDzRq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['f1','f2']],data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,validation_data):\n",
    "        self.x_test = validation_data[0]\n",
    "        self.y_test= validation_data[1]\n",
    "    def on_train_begin(self, logs={}):\n",
    "        ## on begin of training, we are creating a instance varible called history\n",
    "        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n",
    "        self.history={'loss': [],'accuracy': [],'val_loss': [],'val_accuracy': [],'val_recall': [], 'val_auc':[]}\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        true_positives=0\n",
    "        ## on end of each epoch, we will get logs and update the self.history dict\n",
    "        self.history['loss'].append(logs.get('loss'))\n",
    "        self.history['accuracy'].append(logs.get('accuracy'))\n",
    "      \n",
    "        if logs.get('val_loss', -1) != -1:\n",
    "            self.history['val_loss'].append(logs.get('val_loss'))\n",
    "        if logs.get('val_accuracy', -1) != -1:\n",
    "            self.history['val_accuracy'].append(logs.get('val_accuracy'))\n",
    "        # we can get a list of all predicted values at the end of the epoch\n",
    "        # we can use these predicted value and the true values to calculate any custom evaluation score if it is needed for our model\n",
    "        # Here we are taking log of all true positives and then taking average of it\n",
    "        y_pred= self.model.predict(self.x_test)\n",
    "        y_label_pred=np.argmax(y_pred,axis=1)\n",
    "        custom_score = np.log(np.sum(y_test== y_label_pred))/len(y_test)\n",
    "        \n",
    "        #we can also calcualte predefined metrics such as precison, recall, etc. using callbacks \n",
    "        recall = recall_score(y_test,y_label_pred,average='micro')\n",
    "        self.history['val_recall'].append(recall)\n",
    "        print('custom_Score: ',np.round(custom_score,5),'Recall: ',recall)\n",
    "        \n",
    "        auc = roc_auc_score(y_test, y_pred, average='micro')\n",
    "        self.history['val_recall'].append(recall)\n",
    "        print('custom_Score: ',np.round(custom_score,5),'auc: ',auc)\n",
    "        \n",
    "def learning_rate_third_epoch(epoch,lr):\n",
    "    if epoch % 3 == 0:\n",
    "        lr = lr - (lr * 0.05)\n",
    "        return lr\n",
    "    return lr\n",
    "\n",
    "class TerminateNaN(tf.keras.callbacks.Callback):\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        loss = logs.get('loss')\n",
    "        if loss is not None:\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                print(\"Invalid loss and terminated at epoch {}\".format(epoch))\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "# Clear any logs from previous runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "history_own=LossHistory(validation_data=[X_test,y_test])  \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.9, patience=1, min_lr=0.0001)\n",
    "LrScheduler = LearningRateScheduler(learning_rate_third_epoch,verbose=1)\n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss',  verbose=1, save_best_only=True, mode='auto')\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.35, patience=2, verbose=1)\n",
    "terminate_nan = TerminateNaN()\n",
    "log_dir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential([\n",
    "            Dense(10,activation=\"tanh\",input_shape=(2,),kernel_initializer=keras.initializers.RandomUniform(minval=-0, maxval=1)),\n",
    "            Dense(20,activation=\"tanh\",kernel_initializer=keras.initializers.RandomUniform(minval=-0, maxval=1)),\n",
    "            Dense(25,activation=\"tanh\",kernel_initializer=keras.initializers.RandomUniform(minval=-0, maxval=1)),\n",
    "            Dense(15,activation=\"tanh\",kernel_initializer=keras.initializers.RandomUniform(minval=-0, maxval=1)),\n",
    "            Dense(10,activation=\"tanh\",kernel_initializer=keras.initializers.RandomUniform(minval=-0, maxval=1)),\n",
    "            Dense(1, activation='sigmoid')\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                30        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                220       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25)                525       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 15)                390       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                160       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,336\n",
      "Trainable params: 1,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009499999787658453.\n",
      "Epoch 1/20\n",
      "   1/1000 [..............................] - ETA: 36:16 - loss: 0.6953 - accuracy: 0.5625WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0131s). Check your callbacks.\n",
      " 997/1000 [============================>.] - ETA: 0s - loss: 0.6944 - accuracy: 0.5023custom_Score:  0.0019 Recall:  0.495\n",
      "custom_Score:  0.0019 auc:  0.48712371237123714\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.69346, saving model to model_save\\weights-01-0.4897.hdf5\n",
      "1000/1000 [==============================] - 5s 3ms/step - loss: 0.6944 - accuracy: 0.5024 - val_loss: 0.6935 - val_accuracy: 0.4897 - lr: 0.0095\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009499999694526196.\n",
      "Epoch 2/20\n",
      " 998/1000 [============================>.] - ETA: 0s - loss: 0.6935 - accuracy: 0.4984custom_Score:  0.0019 Recall:  0.495\n",
      "custom_Score:  0.0019 auc:  0.5066924192419242\n",
      "\n",
      "Epoch 2: val_loss improved from 0.69346 to 0.69311, saving model to model_save\\weights-02-0.5077.hdf5\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.6935 - accuracy: 0.4983 - val_loss: 0.6931 - val_accuracy: 0.5077 - lr: 0.0095\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009499999694526196.\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.4969custom_Score:  0.0019 Recall:  0.495\n",
      "custom_Score:  0.0019 auc:  0.4930848084808481\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.69311\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.6936 - accuracy: 0.4969 - val_loss: 0.6942 - val_accuracy: 0.4922 - lr: 0.0085\n",
      "Epoch 3: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b2c8648a60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD')\n",
    "model_1.compile(optimizer=optimizer,loss='BinaryCrossentropy',metrics=['accuracy'])\n",
    "callbacks_list = [history_own,reduce_lr,LrScheduler,checkpoint,earlystop,terminate_nan,tensorboard_callback]\n",
    "model_1.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test),batch_size=16,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1424), started 1:33:43 ago. (Use '!kill 1424' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3c27a548cfbbfab1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3c27a548cfbbfab1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-f81d50a69e88>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-f81d50a69e88>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    del ./logs/\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "del ./logs/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRpsCx3NEAtx"
   },
   "source": [
    "<pre>\n",
    "<b>Model-2</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btKuy2SWEFZo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-3tV-KIEFc8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1e2VaqfEEDE"
   },
   "source": [
    "<pre>\n",
    "<b>Model-3</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use he_uniform() as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2M4q3LYEF_1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOaQiRbZEGDU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w41Y3TFENCXk"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<pre>\n",
    "<b>Model-4</b>\n",
    "<pre>\n",
    "1. Try with any values to get better accuracy/f1 score.  \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4agdXzB-DqOj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP3-7U_4LhC6"
   },
   "source": [
    "# Note \n",
    "Make sure that you are plotting tensorboard plots either in your notebook or you can try to create a pdf file with all the tensorboard screenshots.Please write your analysis of tensorboard results for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPci2vqWMN2I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Call_Backs_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
