{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hindu-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joint-potter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open(\"./documents/comp.windows.x_66950.txt\", \"r\")\n",
    "data = fileObject.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def preprocess_email(data):\n",
    "    emails = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", data)\n",
    "#     print(emails)\n",
    "    words= [email.split(sep='@')[1] for email in emails]\n",
    "    words = [item for i in words for item in i.split(sep='.')]\n",
    "    words = [item for item in words if len(item) > 2]\n",
    "    preprocessed_email = [\" \".join(words)]\n",
    "    data = re.sub(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", ' ',data)\n",
    "    return data, preprocessed_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Subject(data):\n",
    "    word= re.findall(\"Subject:.*\", data)\n",
    "    word = [item for item in word[0].split() if ':' not in item]\n",
    "    word = [re.sub('[^a-zA-Z0-9]+', '', _) for _ in word]\n",
    "    preprocessed_subject = [\" \".join(word)]\n",
    "    data = re.sub(\"Subject:.*\",' ', data)\n",
    "    return data, preprocessed_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-identifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_write_from(data):\n",
    "    paragraph = re.sub('From:.*',' ',data)\n",
    "    paragraph = re.sub('Write to:.*',' ',paragraph)\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newline(data):\n",
    "    word = re.sub('\\n',' ',data)\n",
    "    word = re.sub('\\t',' ',word)\n",
    "    word = re.sub('-', ' ',word)\n",
    "    word = re.sub('/',' ' ,word)\n",
    "    word = re.sub(r'\\w+:\\s?',' ' ,word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_brackets(data):\n",
    "    paragraph = data.replace(\"(\",\"<\")\n",
    "    paragraph = paragraph.replace(\")\",\">\")\n",
    "    paragraph = re.sub('<[^>]+>', '', paragraph)\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(data):\n",
    "    data = data.replace('can\\'t','can not')\n",
    "    data = data.replace('\\'s','is')\n",
    "    data = data.replace('i\\'ve','i have')\n",
    "    data = data.replace('i\\'m','i am')\n",
    "    data = data.replace('you\\'re','you are')\n",
    "    data = data.replace('i\\'ll','i will')\n",
    "    data = data.replace('haven\\'t','have not')\n",
    "    return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(data):\n",
    "    chunks = []\n",
    "    chunks = (list(ne_chunk(pos_tag(word_tokenize(data)))))\n",
    "    for chunk in chunks:\n",
    "        if type(chunk) == Tree:\n",
    "            if chunk.label() == 'GPE':\n",
    "                if len(chunk.leaves()) > 1:\n",
    "                    word = \"_\".join([i for i,j in chunk.leaves()])\n",
    "                    for i in range(len(chunk.leaves())): \n",
    "                        if i< len(chunk.leaves())-1:\n",
    "                            data = re.sub(chunk.leaves()[i][0],'',data,flags=re.MULTILINE)\n",
    "                        else:\n",
    "                            data = re.sub(chunk.leaves()[i][0],word,data,flags=re.MULTILINE)\n",
    "            if chunk.label() == 'PERSON':\n",
    "                for term,pog in chunk.leaves():\n",
    "                    data = re.sub(re.escape(term),\"\",data, flags=re.MULTILINE)\n",
    "#                 for i in range(len(chunk.leaves())):\n",
    "#                     data = re.sub(chunk.leaves()[i][0],'',data,flags=re.MULTILINE)   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(data):\n",
    "    data = re.sub('\\d',\"\",data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _word_(data):\n",
    "    data = re.sub(r\"\\b_([a-zA-z]+)_\\b\",r\"\\1\",data)\n",
    "    data = re.sub(r\"\\b_([a-zA-z]+)\\b\",r\"\\1\",data)\n",
    "    data = re.sub(r\"\\b([a-zA-z]+)_\\b\",r\"\\1\",data)\n",
    "    data = re.sub(r\"\\b[a-zA-Z]{1}_([a-zA-Z]+)\",r\"\\1\",data)\n",
    "    data = re.sub(r\"\\b[a-zA-Z]{2}_([a-zA-Z]+)\",r\"\\1\",data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_alpha(data):\n",
    "    data.lower()\n",
    "    data = re.sub(r'\\b\\w{1,2}\\b', '', data)\n",
    "    data = re.sub(r'\\b\\w{15,}\\b', '', data)\n",
    "    data = re.sub(r\"[^a-zA-Z_]\",\" \",data) \n",
    "    data = re.sub(' +', ' ', data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(Input_Text):\n",
    "    \"\"\"Do all the Preprocessing as shown above and\n",
    "    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data\"\"\"\n",
    "    text, list_of_preprocessed_emails = preprocess_email(Input_Text)\n",
    "    text, subject = Subject(text)\n",
    "    text = remove_write_from(text)\n",
    "    text = remove_newline(text)\n",
    "    text = remove_brackets(text)\n",
    "    text = remove_words(text)\n",
    "    text = chunking(text)\n",
    "    text = remove_digits(text)\n",
    "    text = _word_(text)\n",
    "    text = lower_alpha(text)\n",
    "    return (list_of_preprocessed_emails,subject,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(files):\n",
    "    emails_preprocessed = []\n",
    "    sub_preprocessed = []\n",
    "    text_preprocessed = []\n",
    "    for file in files:\n",
    "        list_of_preprocessed_emails,subject,text =preprocess(file)\n",
    "        emails_preprocessed.append(list_of_preprocessed_emails)\n",
    "        sub_preprocessed.append(subject)\n",
    "        text_preprocessed.append(text)\n",
    "    df = pd.DataFrame(list(zip(text_preprocessed, emails_preprocessed,sub_preprocessed)),columns =['text', 'emails','subject'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(os.getcwd())\n",
    "os.chdir(r'C:\\Users\\divya\\PycharmProjects\\Applied_ai\\doc_classification_cnn\\documents')\n",
    "files =[]\n",
    "output = []\n",
    "for file in (os.listdir()):\n",
    "    output.append(file.split('_')[0])\n",
    "    fileObject = open(file, \"r\")\n",
    "    files.append(fileObject.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_preprocessed_emails,subject,text = preprocess_file(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_preprocessed = []\n",
    "sub_preprocessed = []\n",
    "text_preprocessed = []\n",
    "for i in range(len(files)):\n",
    "    list_of_preprocessed_emails,subject,text =preprocess_file(files[i])\n",
    "    emails_preprocessed.append(list_of_preprocessed_emails)\n",
    "    sub_preprocessed.append(subject)\n",
    "    text_preprocessed.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(text_preprocessed, emails_preprocessed,sub_preprocessed,output)),columns =['text', 'emails','subject','label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['concat_string'] = df['subject'].map(lambda x: x[0])+df['emails'].map(lambda x: x[0])+df['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('doc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "italic-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\divya\\PycharmProjects\\Applied_ai\\doc_classification_cnn')\n",
    "df = pd.read_csv('doc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "former-mills",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emails</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "      <th>concat_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Archive atheism resources Alt atheism archive...</td>\n",
       "      <td>['mantis netcom com mantis']</td>\n",
       "      <td>['AltAtheism Atheist Resources']</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>AltAtheism Atheist Resourcesmantis netcom com ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rchive atheism introduction atheism archive i...</td>\n",
       "      <td>['mantis mantis mantis']</td>\n",
       "      <td>['AltAtheism Introduction to Atheism']</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>AltAtheism Introduction to Atheismmantis manti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>article Well has quite different not necessar...</td>\n",
       "      <td>['dbstu1 tu-bs mimsy umd edu umd edu']</td>\n",
       "      <td>['Gospel Dating']</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Gospel Datingdbstu1 tu-bs mimsy umd edu umd ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recently RAs have been ordered apparently pos...</td>\n",
       "      <td>['mantis kepler unh edu']</td>\n",
       "      <td>['university violating separation of churchsta...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>university violating separation of churchstate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article HOWEVER hate economic terrorism and p...</td>\n",
       "      <td>['harder ccr-p ida org harder ccr-p ida org wa...</td>\n",
       "      <td>['socmotss et al Princeton axes matching funds...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>socmotss et al Princeton axes matching funds f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0   Archive atheism resources Alt atheism archive...   \n",
       "1   rchive atheism introduction atheism archive i...   \n",
       "2   article Well has quite different not necessar...   \n",
       "3   Recently RAs have been ordered apparently pos...   \n",
       "4   article HOWEVER hate economic terrorism and p...   \n",
       "\n",
       "                                              emails  \\\n",
       "0                       ['mantis netcom com mantis']   \n",
       "1                           ['mantis mantis mantis']   \n",
       "2             ['dbstu1 tu-bs mimsy umd edu umd edu']   \n",
       "3                          ['mantis kepler unh edu']   \n",
       "4  ['harder ccr-p ida org harder ccr-p ida org wa...   \n",
       "\n",
       "                                             subject        label  \\\n",
       "0                   ['AltAtheism Atheist Resources']  alt.atheism   \n",
       "1             ['AltAtheism Introduction to Atheism']  alt.atheism   \n",
       "2                                  ['Gospel Dating']  alt.atheism   \n",
       "3  ['university violating separation of churchsta...  alt.atheism   \n",
       "4  ['socmotss et al Princeton axes matching funds...  alt.atheism   \n",
       "\n",
       "                                       concat_string  \n",
       "0  AltAtheism Atheist Resourcesmantis netcom com ...  \n",
       "1  AltAtheism Introduction to Atheismmantis manti...  \n",
       "2  Gospel Datingdbstu1 tu-bs mimsy umd edu umd ed...  \n",
       "3  university violating separation of churchstate...  \n",
       "4  socmotss et al Princeton axes matching funds f...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cosmetic-metallic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18875, 43)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "X = np.array(df['concat_string']) \n",
    "Y = to_categorical(np.asarray(df['label'].factorize()[0]))\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "integrated-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "palestinian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 1000\n",
    "tokenizer  = Tokenizer(num_words = MAX_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(X_train.astype(str))\n",
    "tokenize_X_train =  tokenizer.texts_to_sequences(X_train.astype(str))\n",
    "tokenize_X_test =  tokenizer.texts_to_sequences(X_test.astype(str))\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lucky-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_padd_len =1000\n",
    "X_train_pad = pad_sequences(tokenize_X_train, maxlen=max_padd_len)\n",
    "X_test_pad = pad_sequences(tokenize_X_test, maxlen=max_padd_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "early-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word to vector conversion:400000\n"
     ]
    }
   ],
   "source": [
    "glove_name= r'C:\\Users\\divya\\PycharmProjects\\Applied_ai\\doc_classification_cnn/glove.6B.50d.txt'\n",
    "with open(glove_name,\"r\",encoding=\"utf8\") as f:\n",
    "    words= []\n",
    "    word_to_vec= {}\n",
    "    for line in f:\n",
    "        line= line.strip().split()\n",
    "        word_in_sen= line[0]\n",
    "        words.append(word_in_sen)\n",
    "        word_to_vec[word_in_sen]= np.array(line[1:], dtype= 'float64')\n",
    "print(\"Number of word to vector conversion:{}\".format(len(word_to_vec.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tough-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index)+1,50))\n",
    "for word,i in word_index.items():\n",
    "    if word_to_vec.get(word) is not None:\n",
    "        embedding_matrix[i] = word_to_vec[word]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fuzzy-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1, 50, weights=[embedding_matrix], \n",
    "                            input_length=max_padd_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "educational-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(input_shape = (max_padd_len,) , classes = 43):\n",
    "    X_input = Input(input_shape)\n",
    "    embedding = embedding_layer(X_input)\n",
    "    embedding = Reshape((max_padd_len,50))(embedding)\n",
    "    conv_1 = Conv1D(filters=8, kernel_size=3,strides = 4, activation='relu', input_shape=(max_padd_len,100), padding='valid')(embedding)\n",
    "    conv_2 = Conv1D(filters=8, kernel_size=4,strides = 4, activation='relu', input_shape=(max_padd_len,100), padding='valid')(embedding)\n",
    "    conv_3 = Conv1D(filters=8, kernel_size=5,strides = 4,activation='relu', input_shape=(max_padd_len,100), padding='valid')(embedding)\n",
    "    \n",
    "    concat = Concatenate(axis=1)([conv_1, conv_2, conv_3])\n",
    "    \n",
    "    X = MaxPool1D(pool_size=3)(concat)\n",
    "    \n",
    "    conv_4 = Conv1D(filters=8, kernel_size=4,strides = 4, activation='relu', padding='valid')(X)\n",
    "    conv_5 = Conv1D(filters=8, kernel_size=5,strides = 4, activation='relu', padding='valid')(X)\n",
    "    conv_6 = Conv1D(filters=8, kernel_size=6,strides = 4, activation='relu', padding='valid')(X)\n",
    "    \n",
    "    concat = Concatenate(axis=1)([conv_4, conv_5, conv_6])\n",
    "    \n",
    "    X = MaxPool1D(pool_size=3)(concat)\n",
    "    \n",
    "    X = Conv1D(filters=8, kernel_size=5,strides = 4, activation='relu', padding='valid')(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(48, activation='relu')(X)\n",
    "    X = Dense(classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "clear-preservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1000, 50)     4106400     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1000, 50)     0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 250, 8)       1208        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 250, 8)       1608        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 249, 8)       2008        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 749, 8)       0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 249, 8)       0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 62, 8)        264         ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 62, 8)        328         ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 61, 8)        392         ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 185, 8)       0           ['conv1d_3[0][0]',               \n",
      "                                                                  'conv1d_4[0][0]',               \n",
      "                                                                  'conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 61, 8)       0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 15, 8)        328         ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 120)          0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 120)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 48)           5808        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 43)           2107        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,120,451\n",
      "Trainable params: 14,051\n",
      "Non-trainable params: 4,106,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_1(input_shape = (max_padd_len,), classes = y_train.shape[1])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "liberal-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "# Clear any logs from previous runs\n",
    "# history_own=LossHistory(validation_data=[X_test_pad,y_test])  \n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss',  verbose=1, save_best_only=True, mode='auto')\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.35, patience=2, verbose=1)\n",
    "log_dir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "russian-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "callbacks_list = [checkpoint,earlystop,tensorboard_callback]\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',tfa.metrics.F1Score(num_classes=43, average=\"micro\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train,validation_data=(X_test_pad,y_test), epochs=10, batch_size=64,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-august",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
